{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physician Note Classification\n",
    "Deep Learning algorithm development to classify physician notes based on content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load String/NLP Cleaning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DB Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NN Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download English Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ploerch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to locally hosted MIMIC III database and retrieve notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='mimic' user='ploerch' host='localhost'\")\n",
    "cur = conn.cursor()\n",
    "sql = \"select category, text from mimiciii.balanced_view\"\n",
    "results = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Notes for NN\n",
    "#### Tokenize notes and store classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(res, typ):\n",
    "    out = []\n",
    "    if typ == 1:\n",
    "        for index, row in res.iterrows():\n",
    "            out.append(row['category'])\n",
    "    else:\n",
    "        for index, row in res.iterrows():\n",
    "            out.append(word_tokenize(row['text']))\n",
    "    return out\n",
    "\n",
    "words = store_data(results, 2)\n",
    "outcomes = store_data(results, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ploerch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stpwrds = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stem words, Remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_sentences(res, stpwrds, typ):\n",
    "    if typ == 1:\n",
    "        stemmer = PorterStemmer()\n",
    "        out = [[stemmer.stem(nw) for nw in w if nw not in stpwrds] for w in res];\n",
    "    else:\n",
    "        out = []\n",
    "        out = [[nw for nw in w if nw not in stpwrds] for w in res];\n",
    "    return out\n",
    "\n",
    "twords = process_sentences(words, stpwrds, 2)\n",
    "#decided not to stem words in this use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and swap out bigrams in notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_out_bigrams(res):\n",
    "    lower_bigram = Phraser(Phrases(twords, min_count=50, threshold=64))\n",
    "    out = []\n",
    "    for s in twords:\n",
    "        out.append(lower_bigram[s])\n",
    "    return out\n",
    "\n",
    "clean_words = swap_out_bigrams(twords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate unique words and generate unique word index for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_words(res):\n",
    "    \n",
    "    #flatten all word lists\n",
    "    rt = []\n",
    "    for i in res:\n",
    "        if isinstance(i,list): rt.extend(flattern(i))\n",
    "        else: rt.append(i)\n",
    "\n",
    "    #find unique words\n",
    "    out = []\n",
    "    [out.append(w) for w in rt if w not in out];\n",
    "    \n",
    "    return out\n",
    "\n",
    "#Isolate Unique Words\n",
    "uclean_words = find_unique_words(clean_words)\n",
    "\n",
    "#Generate Word Index from Unique Words\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n', oov_token=\"UKN\")\n",
    "tokenizer.fit_on_texts(uclean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use word indices to create X variable (i.e. convert notes to indices for NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_text_tokens(res):\n",
    "    out = []\n",
    "    for w in res:\n",
    "       out.append(' '.join(w))\n",
    "    return out\n",
    "\n",
    "text_tokens = create_text_tokens(clean_words)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(text_tokens) # dim = number db records x number of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Y Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_var(res, X):\n",
    "    uY = np.unique(np.array(res))\n",
    "    Y = np.zeros(shape=(X.shape[0],len(uY)))\n",
    "    #Ynum = np.zeros(len(uY))\n",
    "    idx = 0\n",
    "    for s in uY:\n",
    "        chk = np.array(res)==s\n",
    "        Y[chk,idx]=1\n",
    "        #Ynum[idx] = sum(chk)\n",
    "        idx += 1\n",
    "    return Y\n",
    "\n",
    "Y = create_y_var(outcomes, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test and training datasets (50% training / 50% testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634, 16206)\n",
      "(634, 16206)\n"
     ]
    }
   ],
   "source": [
    "idx = pd.DataFrame(list(range(X.shape[0])))\n",
    "\n",
    "valid_prcnt = .50\n",
    "valid_idx = np.random.choice(idx.index, size=round(valid_prcnt*X.shape[0]), replace=False, p=None)\n",
    "train_idx = ~idx.index.isin(valid_idx)\n",
    "valid_idx = idx.index.isin(valid_idx)\n",
    "\n",
    "x_train = X[train_idx,]\n",
    "y_train = Y[train_idx]\n",
    "\n",
    "x_valid = X[valid_idx,]\n",
    "y_valid = Y[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and train NN\n",
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory name:\n",
    "output_dir = 'model_output/dense' \n",
    "\n",
    "# training parameters:\n",
    "epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# preprocessing: \n",
    "max_note_length = 1000 # Truncate each note to 1000 words\n",
    "pad_type = trunc_type = 'post' # Pad the front so note has 1000 words \n",
    "drop_embed = 0.2\n",
    "\n",
    "# neural network architecture: \n",
    "n_dense = 64 # Number of dense layer nodes\n",
    "n_out = len(np.unique(np.array(outcomes))) # number of output variables to predict\n",
    "dropout = 0.3 # Fraction of nodes to drop out during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_note_length, padding=pad_type, truncating=trunc_type, value=0)\n",
    "x_valid = pad_sequences(x_valid, maxlen=max_note_length, padding=pad_type, truncating=trunc_type, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Dense NN with Textual Data and 30% dropout during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                64064     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13)                845       \n",
      "=================================================================\n",
      "Total params: 69,069\n",
      "Trainable params: 69,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(n_dense, input_dim=max_note_length))\n",
    "model.add(Dense(n_dense, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(n_out, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 634 samples, validate on 634 samples\n",
      "Epoch 1/20\n",
      "634/634 [==============================] - 0s 210us/step - loss: 0.2639 - acc: 0.9232 - val_loss: 0.2442 - val_acc: 0.9240\n",
      "Epoch 2/20\n",
      "634/634 [==============================] - 0s 40us/step - loss: 0.2360 - acc: 0.9257 - val_loss: 0.2237 - val_acc: 0.9279\n",
      "Epoch 3/20\n",
      "634/634 [==============================] - 0s 43us/step - loss: 0.2155 - acc: 0.9289 - val_loss: 0.2061 - val_acc: 0.9317\n",
      "Epoch 4/20\n",
      "634/634 [==============================] - 0s 42us/step - loss: 0.1948 - acc: 0.9324 - val_loss: 0.1902 - val_acc: 0.9342\n",
      "Epoch 5/20\n",
      "634/634 [==============================] - 0s 41us/step - loss: 0.1777 - acc: 0.9367 - val_loss: 0.1738 - val_acc: 0.9420\n",
      "Epoch 6/20\n",
      "634/634 [==============================] - 0s 41us/step - loss: 0.1581 - acc: 0.9461 - val_loss: 0.1585 - val_acc: 0.9471\n",
      "Epoch 7/20\n",
      "634/634 [==============================] - 0s 42us/step - loss: 0.1430 - acc: 0.9511 - val_loss: 0.1457 - val_acc: 0.9515\n",
      "Epoch 8/20\n",
      "634/634 [==============================] - 0s 41us/step - loss: 0.1287 - acc: 0.9547 - val_loss: 0.1349 - val_acc: 0.9529\n",
      "Epoch 9/20\n",
      "634/634 [==============================] - 0s 43us/step - loss: 0.1180 - acc: 0.9586 - val_loss: 0.1240 - val_acc: 0.9572\n",
      "Epoch 10/20\n",
      "634/634 [==============================] - 0s 45us/step - loss: 0.1066 - acc: 0.9630 - val_loss: 0.1134 - val_acc: 0.9619\n",
      "Epoch 11/20\n",
      "634/634 [==============================] - 0s 46us/step - loss: 0.0977 - acc: 0.9653 - val_loss: 0.1041 - val_acc: 0.9638\n",
      "Epoch 12/20\n",
      "634/634 [==============================] - 0s 44us/step - loss: 0.0868 - acc: 0.9702 - val_loss: 0.0948 - val_acc: 0.9666\n",
      "Epoch 13/20\n",
      "634/634 [==============================] - 0s 45us/step - loss: 0.0793 - acc: 0.9721 - val_loss: 0.0863 - val_acc: 0.9699\n",
      "Epoch 14/20\n",
      "634/634 [==============================] - 0s 41us/step - loss: 0.0721 - acc: 0.9752 - val_loss: 0.0794 - val_acc: 0.9720\n",
      "Epoch 15/20\n",
      "634/634 [==============================] - 0s 43us/step - loss: 0.0660 - acc: 0.9766 - val_loss: 0.0737 - val_acc: 0.9736\n",
      "Epoch 16/20\n",
      "634/634 [==============================] - 0s 42us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0689 - val_acc: 0.9756\n",
      "Epoch 17/20\n",
      "634/634 [==============================] - 0s 42us/step - loss: 0.0539 - acc: 0.9805 - val_loss: 0.0647 - val_acc: 0.9769\n",
      "Epoch 18/20\n",
      "634/634 [==============================] - 0s 43us/step - loss: 0.0493 - acc: 0.9819 - val_loss: 0.0603 - val_acc: 0.9788\n",
      "Epoch 19/20\n",
      "634/634 [==============================] - 0s 43us/step - loss: 0.0435 - acc: 0.9845 - val_loss: 0.0560 - val_acc: 0.9807\n",
      "Epoch 20/20\n",
      "634/634 [==============================] - 0s 47us/step - loss: 0.0398 - acc: 0.9863 - val_loss: 0.0524 - val_acc: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3125c940>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y=y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.19.hdf5\") # zero-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a459ae588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat[0:,11:12])\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.31'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:0.2f}\".format(roc_auc_score(y_valid, y_hat)*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the model on all MIMIC III Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract larger MIMIC III notes set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"select category, text from mimiciii.noteevents\"\n",
    "results = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Larger Set of Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-647b2ca9998e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store SQL Results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Store SQL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-5be3dd93e352>\u001b[0m in \u001b[0;36mstore_data\u001b[0;34m(res, typ)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \"\"\"\n\u001b[0;32m-> 1472\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    579\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_toks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tok, **params)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_properties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store SQL Results\n",
    "words = store_data(results, 2)\n",
    "outcomes = store_data(results, 1)\n",
    "print(\"Store SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "twords = process_sentences(words, stpwrds, 2)\n",
    "print(\"Remove Stop Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap out bigrams\n",
    "clean_words = swap_out_bigrams(twords)\n",
    "print(\"Swap Out Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to tokens for NN\n",
    "text_tokens = create_text_tokens(clean_words)\n",
    "print(\"Convert textto tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y variables for Model\n",
    "X_valid = tokenizer.texts_to_matrix(text_tokens) # dim = number db records x number of unique words\n",
    "y_valid = create_y_var(outcomes, X_valid)\n",
    "print(\"Create Model Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess larger set of notes\n",
    "x_valid = pad_sequences(x_valid, maxlen=max_note_length, padding=pad_type, truncating=trunc_type, value=0)\n",
    "print(\"Preprocess larger set of notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the larger MIMIC III dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a361beef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat[0:,11:12])\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-553d09d667a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"{:0.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0my_true_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0my_score_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         score[c] = binary_metric(y_true_c, y_score_c,\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for size 9"
     ]
    }
   ],
   "source": [
    "\"{:0.2f}\".format(roc_auc_score(y_valid, y_hat)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
